diff --git a/src/runner.py b/src/runner.py
index e16329a..d846f29 100644
--- a/src/runner.py
+++ b/src/runner.py
@@ -11,26 +11,17 @@ from ruffus.drmaa_wrapper import run_job, error_drmaa_job
 MEGABYTES_IN_GIGABYTE = 1024
 
 '''
-SLURM options:
+SGE options:
 
---account=name          Charge job to specified accounts
---exclusive             Allocate nodenumber of tasks to invoke on each nodes 
-                        in exclusive mode when cpu consumable resource is
-                        enabled
---mem=MB                Minimum amount of real memory
---mem-per-cpu=MB        Maximum amount of real memory per allocated cpu
-                        required by a job
---mincpus=n             Minimum number of logical processors (threads)
-                        per node
---nodes=N               Number of nodes on which to run (N = min[-max])
---ntasks-per-node=n     Number of tasks to invoke on each node
---partition=partition   Partition requested
---reservation=name      Allocate resources from named reservation
---time=hours:minutes    Set a maximum job wallclock time
---ntasks=n              Number of tasks
---mail-type             Notify user by email when certain event types
-                        occur. Valid type values are BEGIN, END, FAIL,
-                        REQUEUE, and ALL (any state change)
+-account=name          Charge job to specified accounts
+-mem=MB                Minimum amount of real memory
+-mem-per-cpu=MB        Maximum amount of real memory per allocated cpu
+                       required by a job
+-nodes=N               Number of nodes on which to run (N = min[-max])
+-pe -serial=n            Number of tasks to invoke on each node
+-M a	               Notify user by email when certain event types
+                       occur. Valid type values are BEGIN, END, FAIL,
+                       REQUEUE, and ALL (any state change)
 '''
 
 def run_stage(state, stage, command):
@@ -49,12 +40,17 @@ def run_stage(state, stage, command):
     job_name = pipeline_id + '_' + stage
 
     # Generate a "module load" command for each required module
-    module_loads = '\n'.join(['module load ' + module for module in modules])
+    module_loads = '. /etc/profile.d/module.sh\n' + '\n'.join(['module load ' + module for module in modules])
     cluster_command = '\n'.join([module_loads, command])
 
     # Specify job-specific options for SLURM
-    job_options = '--nodes=1 --ntasks-per-node={cores} --ntasks={cores} --time={time} --mem={mem} --partition={queue} --account={account}' \
-                      .format(cores=cores, time=walltime, mem=mem, queue=queue, account=account)
+#    job_options = '--nodes=1 --ntasks-per-node={cores} --ntasks={cores} --time={time} --mem={mem} --partition={queue} --account={account}' \
+#                      .format(cores=cores, time=walltime, mem=mem, queue=queue, account=account)
+
+   #Specify job-specific options for SGE
+    # job_options = 'qsub -pe serial={8} -l hostname=grid-l h_mem={10G} --partition={long.q} -l -M a={ajayi@sanbi.ac.za}' \
+    job_options = '-q {queue} -pe serial {cores} -l docker=1 -l h_vmem={mem}M  -M {account}'.format( cores=cores, queue=queue, mem=mem, account=account) 
+
 
     # Log a message about the job we are about to run
     log_messages = ['Running stage: {}'.format(stage),
@@ -76,8 +72,8 @@ def run_stage(state, stage, command):
                 run_locally = run_local,
                 # Keep a copy of the job script for diagnostic purposes
                 retain_job_scripts = True,
-                retain_stdout = True,
-                retain_stderr = True,
+                #retain_stdout = True,
+                #retain_stderr = True,
                 job_script_directory = state.options.jobscripts, 
                 job_other_options = job_options)
     except error_drmaa_job as err:
